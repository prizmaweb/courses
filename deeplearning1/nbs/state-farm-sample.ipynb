{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function,division\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Header 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "path=\"/home/ubuntu/nbs/data/state/\"\n",
    "sample_path=\"/home/ubuntu/nbs/data/state/sample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The validation set driver images should not be used in training Thus we have to group and copy over images for specific driver images to the validation folder using pandas group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + 'driver_imgs_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the number of drivers: expect 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp = df.groupby('subject')\n",
    "len(gp.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  This function will move images for a fraction of drivers to the validation folder. \n",
    "#### It returns the total number of files moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def create_validation_set(split):\n",
    "    # read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(path + 'driver_imgs_list.csv')\n",
    "    # group the frame by the subject in the image\n",
    "    gp = df.groupby('subject')\n",
    "    # shuffle the list  of subjects\n",
    "    \n",
    "    shuffled_keys = np.random.permutation( gp.groups.keys())\n",
    "    driver_count=len(gp.groups);\n",
    "    validation_count = int(round(split*driver_count))\n",
    "    filecount=0\n",
    "    # pick a fraction of  drivers from the shuffled list\n",
    "    for i in range(validation_count):\n",
    "        subject=shuffled_keys[i]\n",
    "        print('for driver {}'.format(subject))\n",
    "        # get the group associated with the subject\n",
    "        group = gp.get_group(subject)\n",
    "        # loop over the group and move the images into the validation directory\n",
    "        for (subject, cls, img) in group.values:\n",
    "            source = '{}train/{}/{}'.format(path, cls, img)\n",
    "            target = source.replace('train', 'valid')\n",
    "            #print('mv {} {}'.format(source, target))\n",
    "            filecount +=1\n",
    "            os.rename(source, target)\n",
    "    return filecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mkdir data/state/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/data/state/train\n"
     ]
    }
   ],
   "source": [
    "%cd  data/state/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each c* subdirectory, in the main training folder\n",
    "for d in glob('c?'):\n",
    "    # create a sub dir in main valid folder\n",
    "    os.mkdir('../valid/'+d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this will move 20% of the drivers (their images) to the validation set\n",
    "# assumption: each driver image set is of similar size so ~ 20% data moves to validation\n",
    "#adjust the split  if observed count is less in validation\n",
    "moved=create_validation_set(.2)\n",
    "moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd  data/state/train\n",
    "%mkdir ../sample\n",
    "%mkdir ../sample/train\n",
    "%mkdir ../sample/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each c* subdirectory, in the main training folder\n",
    "for d in glob('c?'):\n",
    "    # create a sub dir in sample/training folder\n",
    "    os.mkdir('../sample/train/'+d)\n",
    "    # create a sub dir in sample/valid folder\n",
    "    os.mkdir('../sample/valid/'+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the list of images   from each c sub folder in the main training directory into an array\n",
    "imgs= glob('c?/*.jpg')\n",
    "#Randomly shuffle this list\n",
    "shuffled_imgs = np.random.permutation(imgs)\n",
    "# pick first 1500 files from the shuffled list and copy to the sample/training/c* sub directories\n",
    "for i in range(1500): copyfile(shuffled_imgs[i],'../sample/train/'+shuffled_imgs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# move to the main validation set  directory\n",
    "%cd ../valid    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read and randomly shuffle main validation folder c* sub directory images\n",
    "imgs= glob('c?/*.jpg')\n",
    "shuffled_imgs = np.random.permutation(imgs)\n",
    "# copy first 1000 images over to sample/valid\n",
    "for i in range(1000): copyfile(shuffled_imgs[i],'../sample/valid/'+shuffled_imgs[i])\n",
    "shuffled_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a results directory under the main state folder\n",
    "%cd ../../..\n",
    "%mkdir data/state/results\n",
    "\n",
    "#create a test directory under sample folder\n",
    "%mkdir data/state/sample/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# get the training directoryIterator  from sample/train\n",
    "batches=get_batches(sample_path+'train',batch_size=batch_size)\n",
    "# get the validation  directory iterator from sample/valid .\n",
    "# Batch size double of train(more memory available)\n",
    "val_batches=get_batches(sample_path+'valid',batch_size=batch_size*2,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear model\n",
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 36s - loss: 13.0878 - acc: 0.1387 - val_loss: 13.0909 - val_acc: 0.1790\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 27s - loss: 13.1643 - acc: 0.1740 - val_loss: 12.8194 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb23adf350>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches,\n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is predicting mostly 2-3 classes with 20% validation accuracy. But in past runs, validation accuracy was seen at 9%, with only 1 class being predicted all the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict_generator(batches, batches.N)[:10],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 32s - loss: 13.1248 - acc: 0.1807 - val_loss: 12.6892 - val_acc: 0.2120\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 24s - loss: 13.0386 - acc: 0.1887 - val_loss: 12.7757 - val_acc: 0.2030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb13eb6990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=1e-10\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not much change- let us increase the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 32s - loss: 13.1748 - acc: 0.1780 - val_loss: 12.8156 - val_acc: 0.2050\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 25s - loss: 13.0518 - acc: 0.1853 - val_loss: 12.6816 - val_acc: 0.2130\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 24s - loss: 13.2970 - acc: 0.1733 - val_loss: 12.6664 - val_acc: 0.2140\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 25s - loss: 12.8738 - acc: 0.2000 - val_loss: 12.8091 - val_acc: 0.2050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb13eb6710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr=1e-5\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly, with the drivers being completely different, the training error and validation set error are both high. We are underfitting and need a more complex model.  This is NOT what Jeremy had seen though- but my guess is he had not used different drivers not seen in training.  The new model gets us to 40% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 32s - loss: 2.0536 - acc: 0.3313 - val_loss: 4.3982 - val_acc: 0.1330\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 24s - loss: 1.1208 - acc: 0.6893 - val_loss: 1.8862 - val_acc: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb0b3478d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now we seem to be  headed in the right direction, increase the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 32s - loss: 0.7409 - acc: 0.8347 - val_loss: 1.6571 - val_acc: 0.4700\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.4999 - acc: 0.9160 - val_loss: 1.1364 - val_acc: 0.5740\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.3396 - acc: 0.9613 - val_loss: 0.9279 - val_acc: 0.7230\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.2448 - acc: 0.9827 - val_loss: 0.8295 - val_acc: 0.7470\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.1860 - acc: 0.9927 - val_loss: 0.7549 - val_acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ca254a5d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to a conv model to see if accuracy can get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def with_conv1(batches):\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        #added conv layers\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        #end additions\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)\n",
    "    \n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 36s - loss: 1.6591 - acc: 0.5187 - val_loss: 2.2758 - val_acc: 0.3010\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 27s - loss: 0.3483 - acc: 0.9320 - val_loss: 2.0174 - val_acc: 0.2560\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 36s - loss: 0.0987 - acc: 0.9907 - val_loss: 2.4638 - val_acc: 0.2070\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 29s - loss: 0.0413 - acc: 0.9987 - val_loss: 2.7088 - val_acc: 0.2020\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 31s - loss: 0.0212 - acc: 1.0000 - val_loss: 2.8952 - val_acc: 0.1970\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 28s - loss: 0.0153 - acc: 1.0000 - val_loss: 2.9087 - val_acc: 0.1920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f05f3b619d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation  : shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(sample_path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 38s - loss: 2.1894 - acc: 0.3267 - val_loss: 2.8344 - val_acc: 0.0930\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 30s - loss: 1.1870 - acc: 0.6407 - val_loss: 2.2089 - val_acc: 0.3760\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 36s - loss: 0.7911 - acc: 0.7653 - val_loss: 2.3160 - val_acc: 0.3840\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.6105 - acc: 0.8287 - val_loss: 2.4959 - val_acc: 0.2420\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.4640 - acc: 0.8760 - val_loss: 2.5909 - val_acc: 0.2620\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 32s - loss: 0.3705 - acc: 0.9113 - val_loss: 2.5639 - val_acc: 0.2840\n"
     ]
    }
   ],
   "source": [
    "model = with_conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation: height shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(sample_path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 35s - loss: 1.9241 - acc: 0.4060 - val_loss: 2.2981 - val_acc: 0.2170\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 31s - loss: 0.7790 - acc: 0.7827 - val_loss: 1.9247 - val_acc: 0.2760\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 37s - loss: 0.4189 - acc: 0.8860 - val_loss: 2.4460 - val_acc: 0.1350\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 28s - loss: 0.2960 - acc: 0.9247 - val_loss: 2.7683 - val_acc: 0.1380\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 28s - loss: 0.2098 - acc: 0.9513 - val_loss: 3.0525 - val_acc: 0.1640\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.1478 - acc: 0.9707 - val_loss: 3.1981 - val_acc: 0.2200\n"
     ]
    }
   ],
   "source": [
    "model = with_conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is taken as is from Jeremy's notebook-we apply all data augmentation together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(sample_path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also noticed so far that increasing the learning rate worsened the validation accuracy. We need the original smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 38s - loss: 1.7029 - acc: 0.4807 - val_loss: 3.0242 - val_acc: 0.2380\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.3922 - acc: 0.5373 - val_loss: 2.8365 - val_acc: 0.2190\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 29s - loss: 1.2028 - acc: 0.6173 - val_loss: 2.4380 - val_acc: 0.2750\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 27s - loss: 1.0445 - acc: 0.6700 - val_loss: 2.0852 - val_acc: 0.3190\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.0182 - acc: 0.6820 - val_loss: 1.8515 - val_acc: 0.3580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05c5a14e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the same learning rate and keep going for more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 37s - loss: 0.8896 - acc: 0.7007 - val_loss: 1.9342 - val_acc: 0.3490\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 30s - loss: 0.9068 - acc: 0.6980 - val_loss: 1.4757 - val_acc: 0.5210\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 30s - loss: 0.8038 - acc: 0.7373 - val_loss: 1.1117 - val_acc: 0.6290\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 30s - loss: 0.7483 - acc: 0.7693 - val_loss: 0.9926 - val_acc: 0.6860\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 30s - loss: 0.7198 - acc: 0.7747 - val_loss: 1.0209 - val_acc: 0.6710\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 29s - loss: 0.7312 - acc: 0.7827 - val_loss: 0.7500 - val_acc: 0.7890\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.6303 - acc: 0.8127 - val_loss: 0.6576 - val_acc: 0.7910\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 34s - loss: 0.5909 - acc: 0.8200 - val_loss: 0.5140 - val_acc: 0.8540\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.6204 - acc: 0.8053 - val_loss: 0.4439 - val_acc: 0.8720\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.5983 - acc: 0.8153 - val_loss: 0.3522 - val_acc: 0.9290\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 29s - loss: 0.5368 - acc: 0.8393 - val_loss: 0.4506 - val_acc: 0.8800\n",
      "Epoch 12/25\n",
      "1500/1500 [==============================] - 28s - loss: 0.5021 - acc: 0.8553 - val_loss: 0.3810 - val_acc: 0.8910\n",
      "Epoch 13/25\n",
      "1500/1500 [==============================] - 29s - loss: 0.5131 - acc: 0.8487 - val_loss: 0.3138 - val_acc: 0.9200\n",
      "Epoch 14/25\n",
      "1500/1500 [==============================] - 28s - loss: 0.4911 - acc: 0.8540 - val_loss: 0.2938 - val_acc: 0.9290\n",
      "Epoch 15/25\n",
      "1500/1500 [==============================] - 30s - loss: 0.5065 - acc: 0.8500 - val_loss: 0.2761 - val_acc: 0.9360\n",
      "Epoch 16/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.4838 - acc: 0.8553 - val_loss: 0.2618 - val_acc: 0.9370\n",
      "Epoch 17/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.4420 - acc: 0.8693 - val_loss: 0.3161 - val_acc: 0.9070\n",
      "Epoch 18/25\n",
      "1500/1500 [==============================] - 30s - loss: 0.3994 - acc: 0.8873 - val_loss: 0.2844 - val_acc: 0.9280\n",
      "Epoch 19/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.4096 - acc: 0.8780 - val_loss: 0.2328 - val_acc: 0.9350\n",
      "Epoch 20/25\n",
      "1500/1500 [==============================] - 29s - loss: 0.4165 - acc: 0.8807 - val_loss: 0.2184 - val_acc: 0.9460\n",
      "Epoch 21/25\n",
      "1500/1500 [==============================] - 29s - loss: 0.3907 - acc: 0.8900 - val_loss: 0.2594 - val_acc: 0.9420\n",
      "Epoch 22/25\n",
      "1500/1500 [==============================] - 29s - loss: 0.3722 - acc: 0.8907 - val_loss: 0.2437 - val_acc: 0.9410\n",
      "Epoch 23/25\n",
      "1500/1500 [==============================] - 28s - loss: 0.3369 - acc: 0.8980 - val_loss: 0.2681 - val_acc: 0.9270\n",
      "Epoch 24/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.3613 - acc: 0.8940 - val_loss: 0.2420 - val_acc: 0.9360\n",
      "Epoch 25/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.3235 - acc: 0.9047 - val_loss: 0.2200 - val_acc: 0.9430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05c5a14c90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now verify model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18009 images belonging to 10 classes.\n",
      "Found 4415 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# get the training directoryIterator  from sample/train\n",
    "batches=get_batches(path+'train',batch_size=batch_size)\n",
    "# get the validation  directory iterator from sample/valid .\n",
    "# Batch size double of train(more memory available)\n",
    "val_batches=get_batches(path+'valid',batch_size=batch_size*2,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18009/18009 [==============================] - 306s - loss: 1.2782 - acc: 0.5869 - val_loss: 2.4741 - val_acc: 0.2600\n",
      "Epoch 2/2\n",
      "18009/18009 [==============================] - 292s - loss: 0.6603 - acc: 0.8009 - val_loss: 1.7717 - val_acc: 0.4632\n",
      "Epoch 1/4\n",
      "18009/18009 [==============================] - 297s - loss: 0.4545 - acc: 0.8680 - val_loss: 1.4119 - val_acc: 0.6263\n",
      "Epoch 2/4\n",
      "18009/18009 [==============================] - 290s - loss: 0.3387 - acc: 0.9067 - val_loss: 1.4742 - val_acc: 0.5767\n",
      "Epoch 3/4\n",
      "18009/18009 [==============================] - 293s - loss: 0.2746 - acc: 0.9259 - val_loss: 1.5096 - val_acc: 0.5622\n",
      "Epoch 4/4\n",
      "18009/18009 [==============================] - 295s - loss: 0.2268 - acc: 0.9403 - val_loss: 1.3859 - val_acc: 0.6344\n"
     ]
    }
   ],
   "source": [
    " model = with_conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data augmentation as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18009 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18009/18009 [==============================] - 308s - loss: 0.2003 - acc: 0.9467 - val_loss: 1.4922 - val_acc: 0.5873\n",
      "Epoch 2/5\n",
      "18009/18009 [==============================] - 301s - loss: 0.1771 - acc: 0.9550 - val_loss: 1.3471 - val_acc: 0.6292\n",
      "Epoch 3/5\n",
      "18009/18009 [==============================] - 299s - loss: 0.1500 - acc: 0.9621 - val_loss: 1.3366 - val_acc: 0.6122\n",
      "Epoch 4/5\n",
      "18009/18009 [==============================] - 296s - loss: 0.1383 - acc: 0.9635 - val_loss: 1.2506 - val_acc: 0.6496\n",
      "Epoch 5/5\n",
      "18009/18009 [==============================] - 297s - loss: 0.1318 - acc: 0.9649 - val_loss: 1.1890 - val_acc: 0.6462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf75b2db90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
